Model initialized
Train dataset loaded, with length 9427
Validation dataset loaded, with length 3269
Optimizer initialized
Starting training
Evaluating zero-shot performance...
    Zero-shot performance eval: 0.469256653410829
    Zero-shot performance on train: 0.481
Training on 3143 batches
[EVAL] Timestamp: 07.01.2025 23:34:10, Epoch: 1 / 5
    Evaluating on 1090 batches...
    Batch: 0 / 3143, Batch Loss: 3.562851667404175, Eval loss: 0.8799395185271535
[LOG] Timestamp: 07.01.2025 23:36:38, Epoch: 1 / 5
    Batch: 200 / 3143, Batch Loss: 0.28378623723983765
[LOG] Timestamp: 07.01.2025 23:37:31, Epoch: 1 / 5
    Batch: 400 / 3143, Batch Loss: 0.3564660847187042
[LOG] Timestamp: 07.01.2025 23:38:24, Epoch: 1 / 5
    Batch: 600 / 3143, Batch Loss: 0.14012062549591064
[LOG] Timestamp: 07.01.2025 23:39:17, Epoch: 1 / 5
    Batch: 800 / 3143, Batch Loss: 0.41041627526283264
[EVAL] Timestamp: 07.01.2025 23:40:11, Epoch: 1 / 5
    Evaluating on 1090 batches...
    Batch: 1000 / 3143, Batch Loss: 0.35377323627471924, Eval loss: 0.3707608970480228
[LOG] Timestamp: 07.01.2025 23:42:39, Epoch: 1 / 5
    Batch: 1200 / 3143, Batch Loss: 0.31873396039009094
[LOG] Timestamp: 07.01.2025 23:43:32, Epoch: 1 / 5
    Batch: 1400 / 3143, Batch Loss: 0.2931941747665405
[LOG] Timestamp: 07.01.2025 23:44:25, Epoch: 1 / 5
    Batch: 1600 / 3143, Batch Loss: 0.24861298501491547
[LOG] Timestamp: 07.01.2025 23:45:19, Epoch: 1 / 5
    Batch: 1800 / 3143, Batch Loss: 0.21435301005840302
[EVAL] Timestamp: 07.01.2025 23:46:12, Epoch: 1 / 5
    Evaluating on 1090 batches...
    Batch: 2000 / 3143, Batch Loss: 0.37612250447273254, Eval loss: 0.3315597834116822
[LOG] Timestamp: 07.01.2025 23:48:40, Epoch: 1 / 5
    Batch: 2200 / 3143, Batch Loss: 0.2549666166305542
[LOG] Timestamp: 07.01.2025 23:49:33, Epoch: 1 / 5
    Batch: 2400 / 3143, Batch Loss: 0.7100422978401184
[LOG] Timestamp: 07.01.2025 23:50:27, Epoch: 1 / 5
    Batch: 2600 / 3143, Batch Loss: 0.32969072461128235
[LOG] Timestamp: 07.01.2025 23:51:20, Epoch: 1 / 5
    Batch: 2800 / 3143, Batch Loss: 0.53265380859375
[EVAL] Timestamp: 07.01.2025 23:52:13, Epoch: 1 / 5
    Evaluating on 1090 batches...
    Batch: 3000 / 3143, Batch Loss: 0.3193626403808594, Eval loss: 0.3383202597225478
[EPOCH EVAL] Timestamp: 07.01.2025 23:54:26) Epoch: 1 / 5
    Evaluating on 1090 batches...
    Eval results: 0.640868767207097, Eval loss: 0.3279541974152447
    Eval results on train dataset: 0.64
[EVAL] Timestamp: 08.01.2025 00:00:17, Epoch: 2 / 5
    Evaluating on 1090 batches...
    Batch: 0 / 3143, Batch Loss: 0.43713703751564026, Eval loss: 0.32749233004560163
[LOG] Timestamp: 08.01.2025 00:02:45, Epoch: 2 / 5
    Batch: 200 / 3143, Batch Loss: 0.2956979274749756
[LOG] Timestamp: 08.01.2025 00:03:38, Epoch: 2 / 5
    Batch: 400 / 3143, Batch Loss: 0.32788965106010437
[LOG] Timestamp: 08.01.2025 00:04:31, Epoch: 2 / 5
    Batch: 600 / 3143, Batch Loss: 0.08797099441289902
[LOG] Timestamp: 08.01.2025 00:05:25, Epoch: 2 / 5
    Batch: 800 / 3143, Batch Loss: 0.21495144069194794
[EVAL] Timestamp: 08.01.2025 00:06:18, Epoch: 2 / 5
    Evaluating on 1090 batches...
    Batch: 1000 / 3143, Batch Loss: 0.39385005831718445, Eval loss: 0.30909443302849016
[LOG] Timestamp: 08.01.2025 00:08:46, Epoch: 2 / 5
    Batch: 1200 / 3143, Batch Loss: 0.233742356300354
[LOG] Timestamp: 08.01.2025 00:09:39, Epoch: 2 / 5
    Batch: 1400 / 3143, Batch Loss: 0.3886016309261322
[LOG] Timestamp: 08.01.2025 00:10:33, Epoch: 2 / 5
    Batch: 1600 / 3143, Batch Loss: 0.0751524418592453
[LOG] Timestamp: 08.01.2025 00:11:26, Epoch: 2 / 5
    Batch: 1800 / 3143, Batch Loss: 0.22391442954540253
[EVAL] Timestamp: 08.01.2025 00:12:19, Epoch: 2 / 5
    Evaluating on 1090 batches...
    Batch: 2000 / 3143, Batch Loss: 0.1856144666671753, Eval loss: 0.30919175459020726
[LOG] Timestamp: 08.01.2025 00:14:47, Epoch: 2 / 5
    Batch: 2200 / 3143, Batch Loss: 0.49912044405937195
[LOG] Timestamp: 08.01.2025 00:15:41, Epoch: 2 / 5
    Batch: 2400 / 3143, Batch Loss: 0.2573736011981964
[LOG] Timestamp: 08.01.2025 00:16:34, Epoch: 2 / 5
    Batch: 2600 / 3143, Batch Loss: 0.5874159336090088
[LOG] Timestamp: 08.01.2025 00:17:27, Epoch: 2 / 5
    Batch: 2800 / 3143, Batch Loss: 0.12013906240463257
[EVAL] Timestamp: 08.01.2025 00:18:21, Epoch: 2 / 5
    Evaluating on 1090 batches...
    Batch: 3000 / 3143, Batch Loss: 0.20078711211681366, Eval loss: 0.30707622375851923
[EPOCH EVAL] Timestamp: 08.01.2025 00:20:33) Epoch: 2 / 5
    Evaluating on 1090 batches...
    Eval results: 0.6757418170694401, Eval loss: 0.30509344684216405
    Eval results on train dataset: 0.719
[EVAL] Timestamp: 08.01.2025 00:26:24, Epoch: 3 / 5
    Evaluating on 1090 batches...
    Batch: 0 / 3143, Batch Loss: 0.15555213391780853, Eval loss: 0.30373698035511404
[LOG] Timestamp: 08.01.2025 00:28:52, Epoch: 3 / 5
    Batch: 200 / 3143, Batch Loss: 0.3011227548122406
[LOG] Timestamp: 08.01.2025 00:29:46, Epoch: 3 / 5
    Batch: 400 / 3143, Batch Loss: 0.1555962711572647
[LOG] Timestamp: 08.01.2025 00:30:39, Epoch: 3 / 5
    Batch: 600 / 3143, Batch Loss: 0.16861779987812042
[LOG] Timestamp: 08.01.2025 00:31:32, Epoch: 3 / 5
    Batch: 800 / 3143, Batch Loss: 0.18649886548519135
[EVAL] Timestamp: 08.01.2025 00:32:26, Epoch: 3 / 5
    Evaluating on 1090 batches...
    Batch: 1000 / 3143, Batch Loss: 0.05529788136482239, Eval loss: 0.33010761664513877
[LOG] Timestamp: 08.01.2025 00:34:54, Epoch: 3 / 5
    Batch: 1200 / 3143, Batch Loss: 0.11636274307966232
[LOG] Timestamp: 08.01.2025 00:35:47, Epoch: 3 / 5
    Batch: 1400 / 3143, Batch Loss: 0.4271632432937622
[LOG] Timestamp: 08.01.2025 00:36:41, Epoch: 3 / 5
    Batch: 1600 / 3143, Batch Loss: 0.44309160113334656
[LOG] Timestamp: 08.01.2025 00:37:34, Epoch: 3 / 5
    Batch: 1800 / 3143, Batch Loss: 0.1281387209892273
[EVAL] Timestamp: 08.01.2025 00:38:27, Epoch: 3 / 5
    Evaluating on 1090 batches...
    Batch: 2000 / 3143, Batch Loss: 0.2276870757341385, Eval loss: 0.32005453657769006
[LOG] Timestamp: 08.01.2025 00:40:55, Epoch: 3 / 5
    Batch: 2200 / 3143, Batch Loss: 0.20404283702373505
[LOG] Timestamp: 08.01.2025 00:41:49, Epoch: 3 / 5
    Batch: 2400 / 3143, Batch Loss: 0.3021049201488495
[LOG] Timestamp: 08.01.2025 00:42:42, Epoch: 3 / 5
    Batch: 2600 / 3143, Batch Loss: 0.07391922920942307
[LOG] Timestamp: 08.01.2025 00:43:36, Epoch: 3 / 5
    Batch: 2800 / 3143, Batch Loss: 0.09052642434835434
[EVAL] Timestamp: 08.01.2025 00:44:29, Epoch: 3 / 5
    Evaluating on 1090 batches...
    Batch: 3000 / 3143, Batch Loss: 0.3755077123641968, Eval loss: 0.3178744573786923
[EPOCH EVAL] Timestamp: 08.01.2025 00:46:42) Epoch: 3 / 5
    Evaluating on 1090 batches...
    Eval results: 0.6846130315081065, Eval loss: 0.33063200788205027
    Eval results on train dataset: 0.893
[EVAL] Timestamp: 08.01.2025 00:52:32, Epoch: 4 / 5
    Evaluating on 1090 batches...
    Batch: 0 / 3143, Batch Loss: 0.09131581336259842, Eval loss: 0.333085729142463
[LOG] Timestamp: 08.01.2025 00:55:01, Epoch: 4 / 5
    Batch: 200 / 3143, Batch Loss: 0.2257048636674881
[LOG] Timestamp: 08.01.2025 00:55:54, Epoch: 4 / 5
    Batch: 400 / 3143, Batch Loss: 0.04442683234810829
[LOG] Timestamp: 08.01.2025 00:56:47, Epoch: 4 / 5
    Batch: 600 / 3143, Batch Loss: 0.10762742906808853
[LOG] Timestamp: 08.01.2025 00:57:41, Epoch: 4 / 5
    Batch: 800 / 3143, Batch Loss: 0.0708414688706398
[EVAL] Timestamp: 08.01.2025 00:58:34, Epoch: 4 / 5
    Evaluating on 1090 batches...
    Batch: 1000 / 3143, Batch Loss: 0.0460798405110836, Eval loss: 0.3828803933713662
[LOG] Timestamp: 08.01.2025 01:01:02, Epoch: 4 / 5
    Batch: 1200 / 3143, Batch Loss: 0.015340915881097317
[LOG] Timestamp: 08.01.2025 01:01:56, Epoch: 4 / 5
    Batch: 1400 / 3143, Batch Loss: 0.05645444616675377
[LOG] Timestamp: 08.01.2025 01:02:49, Epoch: 4 / 5
    Batch: 1600 / 3143, Batch Loss: 0.17419922351837158
[LOG] Timestamp: 08.01.2025 01:03:42, Epoch: 4 / 5
    Batch: 1800 / 3143, Batch Loss: 0.029531292617321014
[EVAL] Timestamp: 08.01.2025 01:04:36, Epoch: 4 / 5
    Evaluating on 1090 batches...
    Batch: 2000 / 3143, Batch Loss: 0.0009348659659735858, Eval loss: 0.3951173580481795
[LOG] Timestamp: 08.01.2025 01:07:04, Epoch: 4 / 5
    Batch: 2200 / 3143, Batch Loss: 0.13019223511219025
[LOG] Timestamp: 08.01.2025 01:07:57, Epoch: 4 / 5
    Batch: 2400 / 3143, Batch Loss: 0.062029313296079636
[LOG] Timestamp: 08.01.2025 01:08:51, Epoch: 4 / 5
    Batch: 2600 / 3143, Batch Loss: 0.31364375352859497
[LOG] Timestamp: 08.01.2025 01:09:44, Epoch: 4 / 5
    Batch: 2800 / 3143, Batch Loss: 0.03390094265341759
[EVAL] Timestamp: 08.01.2025 01:10:37, Epoch: 4 / 5
    Evaluating on 1090 batches...
    Batch: 3000 / 3143, Batch Loss: 0.02915661595761776, Eval loss: 0.4379841285225307
[EPOCH EVAL] Timestamp: 08.01.2025 01:12:50) Epoch: 4 / 5
    Evaluating on 1090 batches...
    Eval results: 0.7029672682777608, Eval loss: 0.42238320422072045
    Eval results on train dataset: 0.955
[EVAL] Timestamp: 08.01.2025 01:18:41, Epoch: 5 / 5
    Evaluating on 1090 batches...
    Batch: 0 / 3143, Batch Loss: 0.031689539551734924, Eval loss: 0.4280548250395457
[LOG] Timestamp: 08.01.2025 01:21:09, Epoch: 5 / 5
    Batch: 200 / 3143, Batch Loss: 0.009698373265564442
[LOG] Timestamp: 08.01.2025 01:22:02, Epoch: 5 / 5
    Batch: 400 / 3143, Batch Loss: 0.07717118412256241
[LOG] Timestamp: 08.01.2025 01:22:56, Epoch: 5 / 5
    Batch: 600 / 3143, Batch Loss: 0.012812878005206585
[LOG] Timestamp: 08.01.2025 01:23:49, Epoch: 5 / 5
    Batch: 800 / 3143, Batch Loss: 0.19036148488521576
[EVAL] Timestamp: 08.01.2025 01:24:43, Epoch: 5 / 5
    Evaluating on 1090 batches...
    Batch: 1000 / 3143, Batch Loss: 0.2764609158039093, Eval loss: 0.4901913818118532
[LOG] Timestamp: 08.01.2025 01:27:11, Epoch: 5 / 5
    Batch: 1200 / 3143, Batch Loss: 0.00020683225011453032
[LOG] Timestamp: 08.01.2025 01:28:04, Epoch: 5 / 5
    Batch: 1400 / 3143, Batch Loss: 0.0026419966015964746
[LOG] Timestamp: 08.01.2025 01:28:58, Epoch: 5 / 5
    Batch: 1600 / 3143, Batch Loss: 0.010831375606358051
[LOG] Timestamp: 08.01.2025 01:29:51, Epoch: 5 / 5
    Batch: 1800 / 3143, Batch Loss: 0.2743700444698334
[EVAL] Timestamp: 08.01.2025 01:30:44, Epoch: 5 / 5
    Evaluating on 1090 batches...
    Batch: 2000 / 3143, Batch Loss: 0.0030182183254510164, Eval loss: 0.48138108848038386
[LOG] Timestamp: 08.01.2025 01:33:12, Epoch: 5 / 5
    Batch: 2200 / 3143, Batch Loss: 0.6572698354721069
[LOG] Timestamp: 08.01.2025 01:34:06, Epoch: 5 / 5
    Batch: 2400 / 3143, Batch Loss: 0.11127092689275742
[LOG] Timestamp: 08.01.2025 01:34:59, Epoch: 5 / 5
    Batch: 2600 / 3143, Batch Loss: 0.012317514978349209
[LOG] Timestamp: 08.01.2025 01:35:53, Epoch: 5 / 5
    Batch: 2800 / 3143, Batch Loss: 0.04073842242360115
[EVAL] Timestamp: 08.01.2025 01:36:46, Epoch: 5 / 5
    Evaluating on 1090 batches...
    Batch: 3000 / 3143, Batch Loss: 0.013055261224508286, Eval loss: 0.49472248446351313
[EPOCH EVAL] Timestamp: 08.01.2025 01:38:59) Epoch: 5 / 5
    Evaluating on 1090 batches...
    Eval results: 0.6980728051391863, Eval loss: 0.5141899610787248
    Eval results on train dataset: 0.974
Training finished
Saving model